{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef8426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from collections import Counter  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "#plotting tools\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "#stats module\n",
    "from scipy import stats\n",
    "\n",
    "#MOL2VEC related imports\n",
    "import re, gc\n",
    "from gensim.models import Word2Vec \n",
    "from mol2vec.features import mol2alt_sentence, mol2sentence, MolSentence, DfVec, sentences2vec\n",
    "from gensim.models import word2vec\n",
    "\n",
    "#rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "\n",
    "#ML models and splits from sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_recall_fscore_support,  accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The default xlrd engine has dropped support for xlsx files\n",
    "#REF.:https://stackoverflow.com/questions/65254535/xlrd-biffh-xlrderror-excel-xlsx-file-not-supported\n",
    "xls = pd.ExcelFile('dataset_rdkit.xlsx', engine = 'openpyxl')\n",
    "df1 = pd.read_excel(xls, 'passb1_commontargets')\n",
    "df2 = pd.read_excel(xls, 'passb2_commontargets')\n",
    "df3 = pd.read_excel(xls, 'fail_common_targets')\n",
    "df = pd.concat([df1,df2, df3])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e1fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "\n",
    "#Define clustering setup\n",
    "def ClusterFps(fps,cutoff=0.2):\n",
    "    from rdkit import DataStructs\n",
    "    from rdkit.ML.Cluster import Butina\n",
    "\n",
    "    # first generate the distance matrix:\n",
    "    dists = []\n",
    "    nfps = len(fps)\n",
    "    for i in range(1,nfps):\n",
    "        sims = DataStructs.BulkTanimotoSimilarity(fps[i],fps[:i])\n",
    "        dists.extend([1-x for x in sims])\n",
    "\n",
    "    # now cluster the data:\n",
    "    cs = Butina.ClusterData(dists,nfps,cutoff,isDistData=True)\n",
    "    return cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26243682",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions \n",
    "smi2vec_model = Word2Vec.load('word2vec.model')\n",
    "\n",
    "#wrong ones dict is a dict used to correct a couple of SMI strings.\n",
    "wrong_ones_dict = {\n",
    "    '[H][N]([H])([H])[Pt](Cl)(Cl)[N]([H])([H])[H]' : 'C1CN2C(=NN=C2C(F)(F)F)CN1C(=O)CC(CC3=CC(=C(C=C3F)F)F)N',\n",
    "    '[H][N]([H])([H])[Pt]1(OC(=O)C2(CCC2)C(=O)O1)[N]([H])([H])[H]': 'C1CC(C1)(C(=O)O)C(=O)O.[NH2-].[NH2-].[Pt+2]',\n",
    "    '[H][N]1([H])[C@@H]2CCCC[C@H]2[N]([H])([H])[Pt]11OC(=O)C(=O)O1' : 'C1CCC(C(C1)[NH-])[NH-].C(=O)(C(=O)[O-])[O-].[Pt+4]'\n",
    "}\n",
    "def smi2mol(x):\n",
    "    if x in wrong_ones_dict:\n",
    "        return Chem.MolFromSmiles(wrong_ones_dict[x])\n",
    "    return Chem.MolFromSmiles(x)\n",
    "\n",
    "def smiles2vec(x):\n",
    "    lst = []\n",
    "    for word in re.findall(r'.{3}',x):\n",
    "        if word in smi2vec_model.wv:\n",
    "            lst.append(smi2vec_model.wv[word])\n",
    "    return sum(lst)/len(lst)\n",
    "\n",
    "def get_maccs_from_mol(mol):\n",
    "    return np.array(MACCSkeys.GenMACCSKeys(mol))\n",
    "\n",
    "def read_sheets(sheet_list):\n",
    "    sheet_list = [pd.read_excel(xls, x) for x in sheet_list]\n",
    "    return pd.concat(sheet_list)\n",
    "\n",
    "def atc_splits(df):\n",
    "    in_row = defaultdict(list)\n",
    "    for idx, row in df.iterrows():\n",
    "        for jj in intervention_atc[row['nct_id']]:\n",
    "            in_row[jj].append(idx)\n",
    "\n",
    "    freq_list = list(in_row.items())\n",
    "    freq_list.sort(reverse = True, key = lambda x : len(x[1]))\n",
    "    return freq_list\n",
    "\n",
    "def get_hyperparams(X, y):\n",
    "    classifiers1 = [\n",
    "        GridSearchCV(RandomForestClassifier(n_jobs = -1), {'n_estimators':[100, 200,300,250],'criterion':[\"gini\", \"entropy\"],'max_depth':[10,15,20, None]}, n_jobs  =-1, verbose = 3),\n",
    "        GridSearchCV(ExtraTreesClassifier(n_jobs = -1),{'n_estimators':[100, 200,300,250],'criterion':[\"gini\", \"entropy\"],'max_depth':[10,15,20, None]}, n_jobs  =-1, verbose = 3),\n",
    "        GridSearchCV(AdaBoostClassifier(),{'n_estimators':[100, 200,300,250]}, n_jobs  =-1, verbose = 3),\n",
    "        GridSearchCV(SVC(), {'kernel':['rbf'], 'gamma':[\"scale\"] }, n_jobs  =-1, verbose = 3),\n",
    "        GridSearchCV(LogisticRegression(), {'penalty' : ['l2', 'none']}, n_jobs  =-1),\n",
    "        GridSearchCV(KNeighborsClassifier(), {'n_neighbors': list(range(10)), 'weights':[\"distance\", \"uniform\"]}, n_jobs  =-1, verbose = 3),\n",
    "        GridSearchCV(GaussianNB(), {'var_smoothing' : [1e-9]}, n_jobs  =-1, verbose = 3), \n",
    "        GridSearchCV(MLPClassifier(), {'hidden_layer_sizes' : [[100], [100, 200], [100, 150, 200]], \"learning_rate_init\":[0.01, 0.001, 0.05]}, n_jobs  =-1, verbose = 3)\n",
    "    ]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    classifiers1 = [x.fit(X_scaled, y).best_params_ for x in classifiers1]\n",
    "    return classifiers1\n",
    "\n",
    "def get_hyperparams_all(X, y):\n",
    "    classifiers1 = [\n",
    "        GridSearchCV(RandomForestClassifier(n_jobs = -1), {'n_estimators':[100, 200,300,250],'criterion':[\"gini\", \"entropy\"],'max_depth':[10,15,20, None]}, n_jobs  =-1, verbose = 3),\n",
    "        GridSearchCV(ExtraTreesClassifier(n_jobs = -1),{'n_estimators':[100, 200,300,250],'criterion':[\"gini\", \"entropy\"],'max_depth':[10,15,20, None]}, n_jobs  =-1, verbose = 3),\n",
    "        GridSearchCV(AdaBoostClassifier(),{'n_estimators':[100, 200,300,250]}, n_jobs  =-1, verbose = 3),\n",
    "        GridSearchCV(SVC(), {'kernel':['rbf'], 'gamma':[\"scale\"] }, n_jobs  =-1, verbose = 3),\n",
    "        GridSearchCV(LogisticRegression(), {'penalty' : ['l2', 'none']}, n_jobs  =-1),\n",
    "        GridSearchCV(KNeighborsClassifier(), {'n_neighbors': list(range(20)), 'weights':[\"distance\", \"uniform\"]}, n_jobs  =-1, verbose = 3),\n",
    "        GridSearchCV(GaussianNB(), {'var_smoothing' : [1e-9]}, n_jobs  =-1, verbose = 3), \n",
    "        GridSearchCV(MLPClassifier(), {'hidden_layer_sizes' : [[100], [100, 200], [100, 150, 200]], \"learning_rate_init\":[0.01, 0.001, 0.05]}, n_jobs  =-1, verbose = 3)\n",
    "    ]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    classifiers1 = [x.fit(X_scaled, y) for x in classifiers1]\n",
    "    return classifiers1\n",
    "\n",
    "def get_classifiers(classifiers1):\n",
    "    classifiers = [\n",
    "            #{0:1, 1:5}\n",
    "            RandomForestClassifier(n_jobs = -1, **classifiers1[0],class_weight=\"balanced\"),\n",
    "            ExtraTreesClassifier(n_jobs = -1, **classifiers1[1],class_weight=\"balanced\"),\n",
    "            AdaBoostClassifier(**classifiers1[2]),\n",
    "            SVC(**classifiers1[3],class_weight=\"balanced\"),\n",
    "            LogisticRegression(**classifiers1[4],class_weight=\"balanced\"),\n",
    "            KNeighborsClassifier(**classifiers1[5]),\n",
    "            GaussianNB(**classifiers1[6]),\n",
    "            MLPClassifier(**classifiers1[7])\n",
    "        ]\n",
    "    return classifiers\n",
    "\n",
    "def get_classifiers_weighted(classifiers1):\n",
    "    classifiers = [\n",
    "            #{0:1, 1:5}\n",
    "            RandomForestClassifier(n_jobs = -1, **classifiers1[0],class_weight={1:5, 0:1}),\n",
    "            ExtraTreesClassifier(n_jobs = -1, **classifiers1[1],class_weight={1:5, 0:1}),\n",
    "            AdaBoostClassifier(**classifiers1[2]),\n",
    "            SVC(**classifiers1[3],class_weight={1:5, 0:1}),\n",
    "            LogisticRegression(**classifiers1[4],class_weight={1:5, 0:1}),\n",
    "            KNeighborsClassifier(**classifiers1[5]),\n",
    "            GaussianNB(**classifiers1[6]),\n",
    "            MLPClassifier(**classifiers1[7])\n",
    "        ]\n",
    "    return classifiers\n",
    "\n",
    "def get_basic_classifiers():\n",
    "    classifiers = [\n",
    "            RandomForestClassifier(n_jobs = -1),\n",
    "            ExtraTreesClassifier(n_jobs = -1),\n",
    "            AdaBoostClassifier(),\n",
    "            SVC(),\n",
    "            LogisticRegression(),\n",
    "            KNeighborsClassifier(),\n",
    "            GaussianNB(),\n",
    "            MLPClassifier()\n",
    "        ]\n",
    "    return classifiers\n",
    "\n",
    "def get_train_test_split(X, y, split_indices):\n",
    "    X1_test_set = []\n",
    "    X1_test_set.extend(split_indices[1])\n",
    "    X1_test_mask = np.zeros((X.shape[0],), bool)\n",
    "    X1_test_mask[X1_test_set] = True\n",
    "    X1_test = X[X1_test_mask]\n",
    "    X1 = X[~X1_test_mask]\n",
    "    y1 = y[~X1_test_mask]\n",
    "    y1_test = y[X1_test_mask]\n",
    "    \n",
    "    return X1, X1_test, y1, y1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b5af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invervention name : ATC group mapping\n",
    "intervention_smi = defaultdict(list)\n",
    "for idx, row in df.iterrows():\n",
    "        intervention_smi[row['nct_id']].append(row['smi'])\n",
    "\n",
    "cnt = 0\n",
    "for intervention in intervention_smi:\n",
    "    smi = set(intervention_smi[intervention])\n",
    "    if len(smi) > 1:\n",
    "        cnt+=1\n",
    "assert cnt == 0, \"Intervention with multiple smi strings!!!\"\n",
    "\n",
    "intervention_smi = list(intervention_smi.items())\n",
    "intervention_smi = [(x[0], x[1][0]) for x in intervention_smi]\n",
    "print(intervention_smi[0])\n",
    "fps = [AllChem.GetMorganFingerprintAsBitVect(smi2mol(x[1]),2,1024) for x in intervention_smi]\n",
    "clusters=ClusterFps(fps,cutoff=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ab8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(AllChem.GetMorganFingerprintAsBitVect(smi2mol(intervention_smi[0][1]),2,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905df79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for cutoff in list(np.arange(0.1, 1.5, 0.1)):\n",
    "    clusters=ClusterFps(fps,cutoff=cutoff)\n",
    "    lengths.append(len(clusters))\n",
    "\n",
    "ax = sn.lineplot(x = list(np.arange(0.1, 1.5, 0.1)), y = lengths)\n",
    "ax.set(xlabel='cutoff', ylabel='number of clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b0f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = ClusterFps(fps, cutoff = 0.9)\n",
    "print(f\"The number of clusters: {len(clusters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d4aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([len(i) for i in clusters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9849d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fingerprint functions\n",
    "#accept a dataframe with the 'smi' field, and returns a numpy array of fingerprints for each row of the dataframe.\n",
    "def mol2vec(df):\n",
    "    df = df.copy(deep = True)\n",
    "    df['mol'] = df['smi'].apply(smi2mol)\n",
    "    model = word2vec.Word2Vec.load('../model_300dim.pkl')\n",
    "    #Constructing sentences\n",
    "    df['sentence'] = df.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n",
    "    #Extracting embeddings to a numpy.array\n",
    "    #Note that we always should mark unseen='UNK' in sentence2vec() so that model is taught how to handle unknown substructures\n",
    "    df['mol2vec'] = [DfVec(x) for x in sentences2vec(df['sentence'], model, unseen='UNK')]\n",
    "    X_mol2vec = np.array([x.vec for x in df['mol2vec']])\n",
    "    return X_mol2vec\n",
    "\n",
    "def morgan(df):\n",
    "    df = df.copy(deep = True)\n",
    "    X_mol2vec = np.array([AllChem.GetMorganFingerprintAsBitVect(smi2mol(x),2,1024) for x in df['smi']])\n",
    "    return X_mol2vec\n",
    "\n",
    "def mol2vec_ft(df):\n",
    "    df = df.copy(deep = True)\n",
    "    df['mol'] = df['smi'].apply(smi2mol)\n",
    "    model = word2vec.Word2Vec.load('../updatedmodel.pkl')\n",
    "    #Constructing sentences\n",
    "    df['sentence'] = df.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n",
    "\n",
    "    #Extracting embeddings to a numpy.array\n",
    "    #Note that we always should mark unseen='UNK' in sentence2vec() so that model is taught how to handle unknown substructures\n",
    "    df['mol2vec'] = [DfVec(x) for x in sentences2vec(df['sentence'], model, unseen='UNK')]\n",
    "    X_mol2vec = np.array([x.vec for x in df['mol2vec']])\n",
    "    return X_mol2vec\n",
    "\n",
    "def smi2vec(df):\n",
    "    df = df.copy(deep = True)\n",
    "    df['smi2vec'] = df[\"smi\"].apply(lambda x : smiles2vec(x))\n",
    "    X_smi2vec = np.array([x for x in df['smi2vec']])\n",
    "    return X_smi2vec\n",
    "def maccs(df):\n",
    "    df = df.copy(deep = True)\n",
    "    df['mol'] = df['smi'].apply(smi2mol)\n",
    "    maccs_keys = df['mol'].apply(get_maccs_from_mol)\n",
    "    maccs_keys = maccs_keys.to_numpy()\n",
    "    maccs_keys = np.asarray(list(maccs_keys))\n",
    "    return maccs_keys\n",
    "    \n",
    "## Testing code for all the fingerpring modules\n",
    "for fp in [mol2vec, mol2vec_ft, smi2vec, maccs, morgan]:\n",
    "    x = fp(df)\n",
    "    print(fp.__name__, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7899efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#various sheets of the dataset, for various experiments\n",
    "\n",
    "sheets = {\n",
    "    'main' : ['passb1_commontargets', 'passb2_commontargets', 'fail_common_targets', \"p2fail_commontargets\"],\n",
    "    'ki' : ['passb1_common_ki', 'passb2_common_ki', 'fail_common_ki', \"p2fail_common_ki\"],\n",
    "    'stdval' : ['passb1_common_stdval', 'passb2_common_stdval', 'fail_common_stdval', \"p2fail_common_stdval\"],\n",
    "    'main_v1' : ['passb1_commontargets', 'passb2_commontargets', 'fail_common_targets'],\n",
    "    'ki_v1' : ['passb1_common_ki', 'passb2_common_ki', 'fail_common_ki'],\n",
    "    'stdval_v1' : ['passb1_common_stdval', 'passb2_common_stdval', 'fail_common_stdval'],\n",
    "}\n",
    "\n",
    "#fingerprints to be appended to the datasets using these functions\n",
    "fingerprints = {\n",
    "    'mol2vec': mol2vec,\n",
    "    'mol2vec_ft' : mol2vec_ft,\n",
    "    'smi2vec' : smi2vec,\n",
    "    'maccs' : maccs\n",
    "}\n",
    "\n",
    "#base features to be included in all experiments\n",
    "# base_features = [\"label\",\"smi\",'mol_w', 'num_valence_electrons', 'num_heteroatoms', 'gastiger_charges', 'tpsa', 'h_acceptors',\\\n",
    "#             'h_donors', 'n_rotatable_bonds', 'n_rings', 'num_of_atoms', 'num_of_heavy_atoms', \"count_drug_targets\", \"count_of_approved_trials\"]\n",
    "\n",
    "base_features = [\"label\",\"smi\", \"count_common_targets\", \"count_drug_targets\", \"count_of_approved_trials\", 'gastiger_charges', 'tpsa']\n",
    "#base_features = [\"label\", \"smi\", \"count_common_targets\", \"count_drug_targets\", \"atc\"]\n",
    "\n",
    "experiments = [\n",
    "#     {'data' : 'main', 'fp' : smi2vec, 'additional_features' : [], 'n_clusters' : 8},\n",
    "#     {'data' : 'ki', 'fp' : smi2vec, 'additional_features' : [], 'n_clusters' : 6},\n",
    "#     {'data' : 'ki', 'fp' : smi2vec, 'additional_features' : ['minki'], 'n_clusters' : 6},\n",
    "#     {'data' : 'ki', 'fp' : smi2vec, 'additional_features' : ['minki', 'maxki', 'medianki'], 'n_clusters' : 6},\n",
    "#     {'data' : 'stdval', 'fp' : smi2vec, 'additional_features' : [], 'n_clusters' : 6},\n",
    "#     {'data' : 'stdval', 'fp' : smi2vec, 'additional_features' : ['medianstdval'], 'n_clusters' : 6},\n",
    "#     {'data' : 'stdval', 'fp' : smi2vec, 'additional_features' : ['medianstdval', 'maxstdval', 'minsdtval'], 'n_clusters' : 6},\n",
    "\n",
    "#     {'data' : 'main', 'fp' : maccs, 'additional_features' : [], 'n_clusters' : 8},\n",
    "#     {'data' : 'ki', 'fp' : maccs, 'additional_features' : [], 'n_clusters' : 6},\n",
    "#     {'data' : 'ki', 'fp' : maccs, 'additional_features' : ['minki'], 'n_clusters' : 6},\n",
    "#     {'data' : 'ki', 'fp' : maccs, 'additional_features' : ['minki', 'maxki', 'medianki'], 'n_clusters' : 6},\n",
    "#     {'data' : 'stdval', 'fp' : maccs, 'additional_features' : [], 'n_clusters' : 6},\n",
    "#     {'data' : 'stdval', 'fp' : maccs, 'additional_features' : ['medianstdval'], 'n_clusters' : 6},\n",
    "#     {'data' : 'stdval', 'fp' : maccs, 'additional_features' : ['medianstdval', 'maxstdval', 'minsdtval'], 'n_clusters' : 6},\n",
    "\n",
    "#     {'data' : 'main_v1', 'fp' : smi2vec, 'additional_features' : [], 'n_clusters' : 8},\n",
    "#     {'data' : 'ki_v1', 'fp' : smi2vec, 'additional_features' : [], 'n_clusters' : 6},\n",
    "#     {'data' : 'ki_v1', 'fp' : smi2vec, 'additional_features' : ['minki'], 'n_clusters' : 6},\n",
    "#     {'data' : 'ki_v1', 'fp' : smi2vec, 'additional_features' : ['minki', 'maxki', 'medianki'], 'n_clusters' : 6},\n",
    "#     {'data' : 'stdval_v1', 'fp' : smi2vec, 'additional_features' : [], 'n_clusters' : 6},\n",
    "#     {'data' : 'stdval_v1', 'fp' : smi2vec, 'additional_features' : ['medianstdval'], 'n_clusters' : 6},\n",
    "#     {'data' : 'stdval_v1', 'fp' : smi2vec, 'additional_features' : ['medianstdval', 'maxstdval', 'minsdtval'], 'n_clusters' : 6},\n",
    "    \n",
    "#     {'data' : 'main_v1', 'fp' : maccs, 'additional_features' : [], 'n_clusters' : 8},\n",
    "#     {'data' : 'ki_v1', 'fp' : maccs, 'additional_features' : [], 'n_clusters' : 6},\n",
    "#     {'data' : 'ki_v1', 'fp' : maccs, 'additional_features' : ['minki'], 'n_clusters' : 6},\n",
    "#     {'data' : 'ki_v1', 'fp' : maccs, 'additional_features' : ['minki', 'maxki', 'medianki'], 'n_clusters' : 6},\n",
    "#     {'data' : 'stdval_v1', 'fp' : maccs, 'additional_features' : [], 'n_clusters' : 6},\n",
    "#     {'data' : 'stdval_v1', 'fp' : maccs, 'additional_features' : ['medianstdval'], 'n_clusters' : 6},\n",
    "#     {'data' : 'stdval_v1', 'fp' : maccs, 'additional_features' : ['medianstdval', 'maxstdval', 'minsdtval'], 'n_clusters' : 6},\n",
    "    \n",
    "#     {'data' : 'main', 'fp' : mol2vec, 'additional_features' : [], 'n_clusters' : 8},\n",
    "#     {'data' : 'ki', 'fp' : mol2vec, 'additional_features' : [], 'n_clusters' : 6},\n",
    "#     {'data' : 'ki', 'fp' : mol2vec, 'additional_features' : ['minki'], 'n_clusters' : 6},\n",
    "#     {'data' : 'ki', 'fp' : mol2vec, 'additional_features' : ['minki', 'maxki', 'medianki'], 'n_clusters' : 6},\n",
    "#     {'data' : 'stdval', 'fp' : mol2vec, 'additional_features' : [], 'n_clusters' : 6},\n",
    "#     {'data' : 'stdval', 'fp' : mol2vec, 'additional_features' : ['medianstdval'], 'n_clusters' : 6},\n",
    "#     {'data' : 'stdval', 'fp' : mol2vec, 'additional_features' : ['medianstdval', 'maxstdval', 'minsdtval'], 'n_clusters' : 6},\n",
    "    \n",
    "    \n",
    "#     {'data' : 'main_v1', 'fp' : mol2vec, 'additional_features' : [], 'n_clusters' : 8},\n",
    "#     {'data' : 'ki_v1', 'fp' : mol2vec, 'additional_features' : [], 'n_clusters' : 6},\n",
    "#     {'data' : 'ki_v1', 'fp' : mol2vec, 'additional_features' : ['minki'], 'n_clusters' : 6},\n",
    "    {'data' : 'ki_v1', 'fp' : mol2vec, 'additional_features' : ['minki', 'maxki', 'medianki'], 'n_clusters' : 6},\n",
    "    {'data' : 'stdval_v1', 'fp' : mol2vec, 'additional_features' : [], 'n_clusters' : 6},\n",
    "    {'data' : 'stdval_v1', 'fp' : mol2vec, 'additional_features' : ['medianstdval'], 'n_clusters' : 6},\n",
    "    {'data' : 'stdval_v1', 'fp' : mol2vec, 'additional_features' : ['medianstdval', 'maxstdval', 'minsdtval'], 'n_clusters' : 6},\n",
    "    \n",
    "    {'data' : 'main', 'fp' : mol2vec_ft, 'additional_features' : [], 'n_clusters' : 8},\n",
    "    {'data' : 'ki', 'fp' : mol2vec_ft, 'additional_features' : [], 'n_clusters' : 6},\n",
    "    {'data' : 'ki', 'fp' : mol2vec_ft, 'additional_features' : ['minki'], 'n_clusters' : 6},\n",
    "    {'data' : 'ki', 'fp' : mol2vec_ft, 'additional_features' : ['minki', 'maxki', 'medianki'], 'n_clusters' : 6},\n",
    "    {'data' : 'stdval', 'fp' : mol2vec_ft, 'additional_features' : [], 'n_clusters' : 6},\n",
    "    {'data' : 'stdval', 'fp' : mol2vec_ft, 'additional_features' : ['medianstdval'], 'n_clusters' : 6},\n",
    "    {'data' : 'stdval', 'fp' : mol2vec_ft, 'additional_features' : ['medianstdval', 'maxstdval', 'minsdtval'], 'n_clusters' : 6},\n",
    "    \n",
    "    {'data' : 'main_v1', 'fp' : mol2vec_ft, 'additional_features' : [], 'n_clusters' : 8},\n",
    "    {'data' : 'ki_v1', 'fp' : mol2vec_ft, 'additional_features' : [], 'n_clusters' : 6},\n",
    "    {'data' : 'ki_v1', 'fp' : mol2vec_ft, 'additional_features' : ['minki'], 'n_clusters' : 6},\n",
    "    {'data' : 'ki_v1', 'fp' : mol2vec_ft, 'additional_features' : ['minki', 'maxki', 'medianki'], 'n_clusters' : 6},\n",
    "    {'data' : 'stdval_v1', 'fp' : mol2vec_ft, 'additional_features' : [], 'n_clusters' : 6},\n",
    "    {'data' : 'stdval_v1', 'fp' : mol2vec_ft, 'additional_features' : ['medianstdval'], 'n_clusters' : 6},\n",
    "    {'data' : 'stdval_v1', 'fp' : mol2vec_ft, 'additional_features' : ['medianstdval', 'maxstdval', 'minsdtval'], 'n_clusters' : 6},\n",
    "    \n",
    "    {'data' : 'main', 'fp' : morgan, 'additional_features' : [], 'n_clusters' : 8},\n",
    "    {'data' : 'ki', 'fp' : morgan, 'additional_features' : [], 'n_clusters' : 6},\n",
    "    {'data' : 'ki', 'fp' : morgan, 'additional_features' : ['minki'], 'n_clusters' : 6},\n",
    "    {'data' : 'ki', 'fp' : morgan, 'additional_features' : ['minki', 'maxki', 'medianki'], 'n_clusters' : 6},\n",
    "    {'data' : 'stdval', 'fp' : morgan, 'additional_features' : [], 'n_clusters' : 6},\n",
    "    {'data' : 'stdval', 'fp' : morgan, 'additional_features' : ['medianstdval'], 'n_clusters' : 6},\n",
    "    {'data' : 'stdval', 'fp' : morgan, 'additional_features' : ['medianstdval', 'maxstdval', 'minsdtval'], 'n_clusters' : 6},\n",
    "    \n",
    "    {'data' : 'main_v1', 'fp' : morgan, 'additional_features' : [], 'n_clusters' : 8},\n",
    "    {'data' : 'ki_v1', 'fp' : morgan, 'additional_features' : [], 'n_clusters' : 6},\n",
    "    {'data' : 'ki_v1', 'fp' : morgan, 'additional_features' : ['minki'], 'n_clusters' : 6},\n",
    "    {'data' : 'ki_v1', 'fp' : morgan, 'additional_features' : ['minki', 'maxki', 'medianki'], 'n_clusters' : 6},\n",
    "    {'data' : 'stdval_v1', 'fp' : morgan, 'additional_features' : [], 'n_clusters' : 6},\n",
    "    {'data' : 'stdval_v1', 'fp' : morgan, 'additional_features' : ['medianstdval'], 'n_clusters' : 6},\n",
    "    {'data' : 'stdval_v1', 'fp' : morgan, 'additional_features' : ['medianstdval', 'maxstdval', 'minsdtval'], 'n_clusters' : 6},\n",
    "]\n",
    "\n",
    "df = read_sheets(sheets['ki'])\n",
    "X_m2v = maccs(df)\n",
    "X_m2v.shape\n",
    "kmeans = KMeans(n_clusters=6, random_state = 10).fit(X_m2v)\n",
    "sn.histplot(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aabf985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV to store the final results\n",
    "final_output_csv = None\n",
    "\n",
    "# classifiers used for experiment.\n",
    "names = [\"RF\", \"ExtRa Trees\",  \"Adaboost\",\"SVM\", \"Log. Reg.\", \"kNN\" , \"Naive Bayes\", \"NN\"]\n",
    "def kmeans_based_split(df, n_clusters):\n",
    "    \"\"\"\n",
    "    Splits the df into n subsets based on KMeans clustering of MACCS fingerprints.\n",
    "    Returns a list of dataframes\n",
    "    \"\"\"\n",
    "    #print(len(df))\n",
    "    X_maccs = maccs(df.copy())\n",
    "    kmeans = KMeans(n_clusters=n_clusters).fit(X_maccs)\n",
    "    labels = kmeans.labels_\n",
    "    assert labels.shape[0] == len(df)\n",
    "    dataframes = []\n",
    "    for i in range(n_clusters):\n",
    "        df_subset = df.loc[labels == i].copy()\n",
    "        dataframes.append(df_subset)\n",
    "    #print([len(i) for i in dataframes], sum([len(i) for i in dataframes]))\n",
    "    assert sum([len(i) for i in dataframes]) == len(df)\n",
    "    return dataframes\n",
    "    \n",
    "for experiment in experiments: \n",
    "    print(experiment)\n",
    "    \n",
    "    #output csv for this experiment\n",
    "    output_csv = {'intervention':[], 'train_size' : [], 'test_size' : [], 'confusion' : [],\\\n",
    "                  'auc' : []}\n",
    "    for i in \"aprf\":\n",
    "        for j in names:\n",
    "            output_csv[i+\"_\"+j] = []\n",
    "            if i != 'a':\n",
    "                output_csv[i+\"0_\"+j] = []\n",
    "            \n",
    "            \n",
    "    print(\"Experiment:\", experiment)\n",
    "    df = read_sheets(sheets[experiment['data']])\n",
    "    # Drop nan values from required columns\n",
    "    df = df.loc[:, df.columns.isin(base_features+ experiment['additional_features'] + [\"minsdtval\", \"minki\", \"drugbank id\", \"count_common_targets\"])].dropna()\n",
    "    df = df.loc[:, df.columns.isin(base_features+ experiment['additional_features'] + [\"drugbank id\", \"count_common_targets\"])].dropna()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = df[df['count_common_targets'] <=6]\n",
    "    experiment['n_passes'] = len(df[df['label'] == 0])\n",
    "    experiment['n_fails'] = len(df[df['label'] == 1])\n",
    "    \n",
    "    splits =  kmeans_based_split(df, experiment['n_clusters'])\n",
    "    df = None\n",
    "    y = [df.loc[:, df.columns.isin(['label'])] for df in splits]\n",
    "    y = [df.iloc[:, 0].to_numpy() for df in y]\n",
    "    sample = splits[0].loc[:, splits[0].columns.isin(base_features [2:] + experiment['additional_features'] + [\"drugbank id\"])]\n",
    "    print(sample.columns)\n",
    "    X = [df.loc[:, df.columns.isin(base_features [2:] + experiment['additional_features'] + [\"drugbank id\"])].to_numpy() for df in splits]\n",
    "    assert X[0].shape[1] == len(base_features[2:] + experiment['additional_features']+ ['drugbank id']), \"SOME FEATURES MISSING!!!!\"\n",
    "    assert y[0].shape[0] == X[0].shape[0], \"missing rows!\"\n",
    "\n",
    "    for i in range(experiment['n_clusters']):\n",
    "        X[i] = np.concatenate([X[i], experiment['fp'](splits[i])], 1)\n",
    "    \n",
    "    # obtain the best hyperparameters for current setup via grid search.\n",
    "    classifiers1 = get_hyperparams(np.concatenate(X, axis = 0)[:,1:], np.concatenate(y, axis = 0))\n",
    "    print(classifiers1)\n",
    "    kfolds = [StratifiedKFold(n_splits = 5).split(X[i], y[i]) for i in range(experiment['n_clusters'])]\n",
    "    \n",
    "    for split in tqdm(range(5)):\n",
    "        gc.collect()\n",
    "        X_trains, X_tests, y_trains, y_tests = [], [], [], []\n",
    "        \n",
    "        for i in range(experiment['n_clusters']):\n",
    "            train_idx, test_idx = next(kfolds[i])\n",
    "            X_trains.append(X[i][train_idx])\n",
    "            X_tests.append(X[i][test_idx])\n",
    "            y_trains.append(y[i][train_idx])\n",
    "            y_tests.append(y[i][test_idx])\n",
    "            \n",
    "        \n",
    "        X_train = np.concatenate(X_trains, 0)\n",
    "        X_test = np.concatenate(X_tests, 0)\n",
    "        y_train = np.concatenate(y_trains, 0)\n",
    "        y_test = np.concatenate(y_tests, 0)\n",
    "        \n",
    "        train_drugs = X_train[:,0].tolist()\n",
    "        test_mask = []\n",
    "        for i in range(X_test.shape[0]):\n",
    "            test_mask.append(X_test[i][0] not in train_drugs)\n",
    "        X_train = X_train[:,1:]\n",
    "        \n",
    "        test_mask = np.asarray(test_mask, dtype = bool)\n",
    "        X_test = X_test[test_mask]\n",
    "        y_test = y_test[test_mask]\n",
    "        cnt_mask = np.where(X_test[:, 1] == 1)\n",
    "        X_test = X_test[cnt_mask]\n",
    "        y_test = y_test[cnt_mask]\n",
    "        X_test = X_test[:,1:]\n",
    "        print(X_train.shape, X_test.shape)\n",
    "        \n",
    "        classifiers = get_classifiers(classifiers1)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        accuracy_values = []\n",
    "        pvalues = []\n",
    "        rvalues = []\n",
    "        fvalues = []\n",
    "        \n",
    "        pvalues0 = []\n",
    "        rvalues0 = []\n",
    "        fvalues0 = []\n",
    "        \n",
    "        nmes = []\n",
    "        logreg_confusion = None\n",
    "        logreg_auc = None\n",
    "        \n",
    "        for idx, clf in enumerate(classifiers):\n",
    "            clf.fit(X_train, y_train)\n",
    "            pred = clf.predict(X_test)\n",
    "            accuracy_values.append(\n",
    "                accuracy_score(y_test, pred)\n",
    "            )\n",
    "            p, r, f, _ = precision_recall_fscore_support(y_test, pred, pos_label = 0, average = \"binary\")\n",
    "            pvalues.append(p)\n",
    "            rvalues.append(r)\n",
    "            fvalues.append(f)\n",
    "            p0, r0, f0, _ = precision_recall_fscore_support(y_test, pred, pos_label = 1, average = \"binary\")\n",
    "            pvalues0.append(p0)\n",
    "            rvalues0.append(r0)\n",
    "            fvalues0.append(f0)\n",
    "            nmes.append(names[idx])\n",
    "            \n",
    "            if names[idx] == \"Log. Reg.\":\n",
    "                logreg_confusion = confusion_matrix(y_test, pred).__str__()\n",
    "                pred = clf.predict_proba(X_test)\n",
    "                try:\n",
    "                    logreg_auc = roc_auc_score(y_test, pred[:,0])\n",
    "                except ValueError:\n",
    "                    logreg_auc = -1\n",
    "        \n",
    "        acc_dict = {\n",
    "            \"a\" : accuracy_values,\n",
    "            \"p\" : pvalues, \n",
    "            \"r\" : rvalues, \n",
    "            \"f\" : fvalues,\n",
    "            \"p0\" : pvalues0, \n",
    "            \"r0\" : rvalues0, \n",
    "            \"f0\" : fvalues0,\n",
    "            \"algorithm\" : nmes\n",
    "        }\n",
    "        \n",
    "        output_csv[\"confusion\"].append(logreg_confusion)\n",
    "        output_csv[\"auc\"].append(logreg_auc)\n",
    "        output_csv[\"intervention\"].append(split)\n",
    "        output_csv['train_size'].append(X_train.shape[0])\n",
    "        output_csv['test_size'].append(X_test.shape[0])\n",
    "        for ii in names:\n",
    "            for jj in \"arpf\":\n",
    "                output_csv[jj+\"_\"+ii].append(acc_dict[jj][acc_dict[\"algorithm\"].index(ii)])\n",
    "                if jj != 'a':\n",
    "                    output_csv[jj+\"0_\"+ii].append(acc_dict[jj+'0'][acc_dict[\"algorithm\"].index(ii)])\n",
    "        \n",
    "    output_csv['intervention'].append(\"mean\")\n",
    "    output_csv['confusion'].append(\"mean\")\n",
    "\n",
    "    for i in output_csv:\n",
    "        if i not in [\"intervention\", \"confusion\"]:\n",
    "            output_csv[i].append(sum(output_csv[i])/len(output_csv[i]))\n",
    "\n",
    "    output_csv['intervention'].append(\"std\")\n",
    "    output_csv['confusion'].append(\"std\")\n",
    "    for i in output_csv:\n",
    "        if i not in [\"intervention\", \"confusion\"]:\n",
    "            output_csv[i].append(np.std(np.array(output_csv[i][:-1])))\n",
    "    for i in output_csv:\n",
    "        if i != \"intervention\":\n",
    "            output_csv[i].extend([np.nan]*15)\n",
    "        else:\n",
    "            output_csv[i].extend([\"data : %s, fp : %s, additional_features : %s, passes, fails : %d, %d\"%(experiment['data'], \\\n",
    "                            experiment['fp'].__name__, \",\".join(experiment['additional_features'])\n",
    "                            , experiment['n_passes'], experiment['n_fails'])] + [np.nan]*14)\n",
    "\n",
    "    if final_output_csv is None:\n",
    "        final_output_csv = deepcopy(output_csv)\n",
    "    else:\n",
    "        for i in output_csv:\n",
    "            final_output_csv[i].extend(output_csv[i])\n",
    "    df = pd.DataFrame(final_output_csv)\n",
    "    df.to_csv(\"cluster_experiments_cnt_one.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
