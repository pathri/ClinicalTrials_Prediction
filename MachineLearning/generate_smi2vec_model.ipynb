{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook used to generate a Smi2Vec model from a corpus of SMI strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "import random \n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os \n",
    "import random\n",
    "import seaborn as sns \n",
    "\n",
    "\n",
    "class SPVec:\n",
    "\n",
    "    def __init__(self,filename):\n",
    "        self.filename = filename\n",
    "   \n",
    "    def read_data(self):\n",
    "        data=pd.read_csv(self.filename)\n",
    "        return data\n",
    "\n",
    "    def word2vec(self,dims,window_size,negative_size):\n",
    "        word_vec = pd.DataFrame()\n",
    "        dictionary=[]\n",
    "        Index = []\n",
    "        data=self.read_data()\n",
    "        texts = [[word for word in re.findall(r'.{3}',document)] for document in list(data)]\n",
    "        model = Word2Vec(texts,size=dims,window=window_size,min_count=1,negative=negative_size,sg=1,sample=0.001,hs=1,workers=4)\n",
    "        vectors = pd.DataFrame([model[word] for word in (model.wv.vocab)])\n",
    "        vectors['Word'] = list(model.wv.vocab)\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            Index.append(i)\n",
    "        # Word segmentation\n",
    "        for i in range(len(texts)):\n",
    "            i_word=[]         \n",
    "            for w in range(len(texts[i])):\n",
    "                i_word.append(Index[i])    \n",
    "            dictionary.extend(i_word)\n",
    "        word_vec['Id'] = dictionary\n",
    "        \n",
    "        # word vectors generation\n",
    "        dictionary=[]\n",
    "        for i in range(len(texts)):\n",
    "            i_word=[]         \n",
    "            for w in range(len(texts[i])):\n",
    "                i_word.append(texts[i][w])    \n",
    "            dictionary.extend(i_word)\n",
    "        word_vec['Word'] = dictionary\n",
    "        del dictionary,i_word\n",
    "        word_vec = word_vec.merge(vectors,on='Word', how='left')\n",
    "        #word_vec = word_vec.drop('Word',axis=1)\n",
    "        word_vec.columns = ['Id']+['word']+[\"vec_{0}\".format(i) for i in range(0,dims)]\n",
    "\n",
    "        return word_vec\n",
    "\n",
    "    #Molecular Structure and Protein Sequence Representation\n",
    "    def feature_embeddings(self,dims):\n",
    "        word_vec = self.word2vec(dims,window_size,negative_size)\n",
    "        word_vec=word_vec.drop('Word',axis=1)\n",
    "        name = [\"vec_{0}\".format(i) for i in range(0,dims)]\n",
    "        feature_embeddings = pd.DataFrame(word_vec.groupby(['Id'])[name].agg('mean')).reset_index()\n",
    "        feature_embeddings.columns=[\"Index\"]+[\"mean_ci_{0}\".format(i) for i in range(0,dims)]\n",
    "        return feature_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mols.smi\", \"r\") as f:\n",
    "    l = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CC[C@H](C)[C@H](NC(=O)[C@H](CCC(O)=O)NC(=O)[C@H](CCC(O)=O)NC(=O)[C@H](CC1=CC=CC=C1)NC(=O)[C@H](CC(O)=O)NC(=O)CNC(=O)[C@H](CC(N)=O)NC(=O)CNC(=O)CNC(=O)CNC(=O)CNC(=O)[C@@H]1CCCN1C(=O)[C@H](CCCNC(N)=N)NC(=O)[C@@H]1CCCN1C(=O)[C@H](N)CC1=CC=CC=C1)C(=O)N1CCC[C@H]1C(=O)N[C@@H](CCC(O)=O)C(=O)N[C@@H](CCC(O)=O)C(=O)N[C@@H](CC1=CC=C(O)C=C1)C(=O)N[C@@H](CC(C)C)C(O)=O'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11172\n"
     ]
    }
   ],
   "source": [
    "data = [x.strip() for x in l]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dims,window_size,negative_size = 100,6,12\n",
    "texts = [[word for word in re.findall(r'.{3}',document)] for document in list(data)]\n",
    "model = Word2Vec(texts,size=dims,window=window_size,min_count=1,negative=negative_size,sg=1,sample=0.001,hs=1,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-950c714f1fed>:1: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \"CC[\" in model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"CC[\" in model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
